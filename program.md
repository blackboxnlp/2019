
Summary of the program
----------------------

| Time         | Program item                                  |
|--------------|-----------------------------------------------|
| 09:00-09:10  | Opening remarks                               |
| 09:10-10:00  | Invited talk 1: Yoav Goldberg                 |
| 10:00-11:00  | Poster session 1 (10:30-11 tea break)         |
| 11:00-12:30  |  Oral presentation session 1 (6 x 15 minutes) |
| 12:30-14:00  | Lunch                                         |
| 14:00-14:50  | Invited talk 2: Graham Neubig                 |
| 14:50-16:00  | Poster session 2 (15:30-16 tea break)         |
| 16:00-16:50  | Invited talk 3: Leila Wehbe                   |
| 16:50-17:20  |  Oral presentation session 2 (2 x 15 minutes) |
| 17:20-17:30  | Best paper announcement and closing remarks   |


Oral presentation session 1
---------------------------

- 11:00-11:15 Interpretable Structure Induction via Sparse Attention. Ben Peters, Vlad Niculae and André F. T. Martins - 11:15-11:30 Understanding Convolutional Neural Networks for Text Classification. Alon Jacovi, Yoav Goldberg and Oren Sar Shalom 
- 11:30-11:45 Extracting Syntactic Trees from Transformer Encoder Self-Attentions. David Mareček and Rudolf Rosa 
- 11:45-12:00 Context-Free Transductions with Neural Stacks. Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz and Simon Mendelsohn 
- 12:00-12:15 Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures. Danilo Croce, Daniele Rossini and Roberto Basili 
- 12:15-12:30 Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell. Pia Sommerauer and Antske Fokkens 

Oral presentation session 2
---------------------------

- 16:50-17:05 What do RNN Language Models Learn about Filler–Gap Dependencies? Ethan Wilcox, Roger Levy, Takashi Morita and Richard Futrell 
- 17:05-17:20 Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information. Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes and Willem Zuidema 

Poster session 1
----------------
- When does deep multi-task learning work for loosely related document classification tasks?	Emma Kerinec, Chloé Braud and Anders Søgaard
- Analyzing Learned Representations of a Deep ASR Performance Prediction Model.	zied elloumi, Laurent Besacier, Olivier Galibert and Benjamin Lecouteux
- Learning Explanations from Language Data.	David Harbecke, Robert Schwarzenberg and Christoph Alt
- Nightmare at test time: How punctuation prevents parsers from generalizing.	Anders Søgaard, Miryam de Lhoneux and Isabelle Augenstein
- How much should you ask? On the question structure in QA systems.	Barbara Rychalska, Dominika Basaj, Anna Wróblewska and Przemyslaw Biecek
- Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System.	Barbara Rychalska, Dominika Basaj, Anna Wróblewska and Przemyslaw Biecek
- Interpretable Textual Neuron Representations for NLP.	Nina Poerner, Benjamin Roth and Hinrich Schütze
- Evaluating Textual Representations through Image Generation.	Graham Spinks and Marie-Francine Moens
- On the Role of Document Preprocessing in Neural Text Classification.	Jose Camacho-Collados and Mohammad Taher Pilehvar
- Language Models Learn POS First.	Naomi Saphra and Adam Lopez
- Jump to better conclusions: SCAN both left and right.	Joost Bastings, Marco Baroni, Jason Weston, Kyunghyun Cho and Douwe Kiela
- Ellipsis Resolution in Neural Networks.	Ola Rønning, Daniel Hardt and Anders Søgaard
- Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder models.	Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki Okazaki, Kentaro Inui and Masaaki Nagata
- Rule induction for global explanation of trained models.	Madhumita Sushil, Simon Suster and Walter Daelemans
- Predicting and interpreting embeddings for out of vocabulary words in downstream tasks.	Nicolas Garneau, Jean-Samuel Leboeuf and Luc Lamontagne
- Can LSTM Learn to Capture Agreement? The Case of Basque.	Shauli Ravfogel, Yoav Goldberg and Francis Tyers
- Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks.	Joao Loula, Marco Baroni and Brenden Lake
- Probing sentence embeddings for structure-dependent tense.	Geoff Bacon and Terry Regier
Deep Representations of Language Are Disentangled into Interpretable Linguistic Concepts.	Seil Na, Yo Joong Choe, Dong-Hyun Lee and Gunhee Kim
- Evaluating the Ability of LSTMs to Learn Context-Free Grammars.	Luzi Sennhauser and Robert Berwick
- Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation.	Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White and Benjamin Van Durme
- Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style.	Reid Pryzant, Sugato Basu and Kazoo Sone
- Interpretable Word Embedding Contextualization.	Kyoung-Rok Jang and Sung-Hyon Myaeng
- Interpreting Neural Networks with Nearest Neighbors.	Eric Wallace, Shi Feng and Jordan Boyd-Graber
- 'Indicatements' that character language models learn English morpho-syntactic units and regularities.	Yova Kementchedjhieva and Adam Lopez

Poster session 2
----------------

- State Gradients for RNN Memory Analysis.	Lyan Verwimp, Hugo Van hamme, Vincent Renkens and Patrick Wambacq
- LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation.	Pankaj Gupta and Hinrich Schütze
- Analysing the potential of seq-to-seq models for incremental interpretation in task-oriented dialogue.	Dieuwke Hupkes, Sanne Bouwmeester and Raquel Fernández
- An Operation Sequence Model for Explainable Neural Machine Translation.	Felix Stahlberg and Bill Byrne
- Introspection for convolutional automatic speech recognition.	Andreas Krug and Sebastian Stober
- Portable, layer-wise task performance monitoring for NLP models.	Tom Lippincott
- GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.	Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy and Samuel Bowman
- Explicitly modeling case improves neural dependency parsing.	Clara Vania and Adam Lopez
- Learning and Evaluating Sparse Interpretable Sentence Embeddings.	Valentin Trifonov, Octavian-Eugen Ganea and Anna Potapenko
- Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis.	Kelly Zhang and Samuel Bowman
- Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items.	Jaap Jumelet and Dieuwke Hupkes
- Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model.	Steven Derby, Paul Miller, Brian Murphy and Barry Devereux
- Closing Brackets with Recurrent Neural Networks.	Natalia Skachkova, Thomas Trost and Dietrich Klakow
- Iterative Recursive Attention Model for Interpretable Sequence Classification.	Martin Tutek and Jan Šnajder
- Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models.	Avery Hiebert, Cole Peterson, Alona Fyshe and Nishant Mehta
- Debugging Sequence-to-Sequence Models with Seq2Seq-Vis.	Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister and Alexander Rush
- Grammar Induction with Neural Language Models: An Unusual Replication.	Phu Mon Htut, Kyunghyun Cho and Samuel Bowman
- Importance of Self-Attention for Sentiment Analysis.	Gaël Letarte, Frédérik Paradis, Philippe Giguère and François Laviolette
- Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?	Prajit Dhar and Arianna Bisazza
- Diagnosing Failures in Question Answering Tasks with Attention.	Aida Nematzadeh, Kaylee Burns, Erin Grant and Tom Griffiths
- An Analysis of Encoder Representations in Transformer-Based Machine Translation.	Alessandro Raganato and Jörg Tiedemann
- End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space.	Pranava Swaroop Madhyastha, Josiah Wang and Lucia Specia
- Evaluating Grammaticality in Seq2seq Models with a Broad Coverage HPSG Grammar: A Case Study on Machine Translation.	Johnny Wei, Khiem Pham, Brendan O'Connor and Brian Dillon
- Limitations in learning an interpreted language with recurrent models.	Denis Paperno

