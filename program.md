
[Main page](index.md)

Summary of the program
----------------------

| Time         | Program item                                  |
|--------------|-----------------------------------------------|
| 09:00-09:10  | Opening remarks                               |
| 09:10-10:00  | Invited talk 1: [Arianna Bisazza](index.md#arianna-bisazza)|
| 10:00-11:15  | Poster session 1 (10:30-11 tea break)         |
| 11:15-12:30  | Oral presentation session 1 (5 x 15 minutes)  |
| 12:30-14:00  | Lunch                                         |
| 14:00-14:50  | Invited talk 2: [Michael F. Bonner](index.md#michael-f-bonner)|
| 14:50-16:00  | Poster session 2 (15:30-16 tea break)         |
| 16:00-16:45  | Oral presentation session 2 (3 x 15 minutes)  |
| 16:45-17:30  | Panel discussion                              |
| 17:20-17:30  | Best paper announcement and closing remarks   |


Oral presentation session 1
---------------------------
- 11:15-11:30 _Character Eyes: Seeing Language through Character-Level Taggers_. Yuval Pinter, Marc Marone and Jacob Eisenstein.
- 11:30-11:45 _Faithful Multimodal Explanation for Visual Question Answering_. Jialin Wu and Raymond Mooney. 
- 11:45-12:00 _Evaluating Recurrent Neural Network Explanations_. Leila Arras, Ahmed Osman, Klaus-Robert Müller and Wojciech Samek.
- 12:00-12:15 _On the Realization of Compositionality in Neural Networks_. Joris Baan, Jana Leible, Mitja Nikolaus, David Rau, Dennis Ulmer, Tim Baumgärtner, Dieuwke Hupkes and Elia Bruni.
- 12:15-12:30 _Learning the Dyck Language with Attention-based Seq2Seq Models_. Xiang Yu, Ngoc Thang Vu and Jonas Kuhn.

Oral presentation session 2
---------------------------

- 16:00-16:15 _GEval: Tool for Debugging NLP Datasets and Models_. Filip Graliński, Anna Wróblewska, Tomasz Stanisławek, Kamil Grabowski and Tomasz Górecki
- 16:15-16:30 _From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self- Attentions_. David Mareček and Rudolf Rosa. 
16:30-16:45 _What does BERT look at? An Analysis of BERT’s Attention_. Kevin Clark, Urvashi Khandelwal, Omer Levy and Christopher D. Manning. 


Poster session 1
----------------
Archival papers:
- _Transcoding compositionally: using attention to find more generalizable solutions_. Kris Korrel, Dieuwke Hupkes, Verna Dankers and Elia Bruni
- _Sentiment analysis is not solved! Assessing and probing sentiment classification_. Jeremy Barnes, Lilja Øvrelid and Erik Velldal
- _Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling_. Dominik Schlechtweg, Cennet Oguz and Sabine Schulte im Walde
- _Can neural networks understand monotonicity reasoning?_. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze and Johan Bos
- _Multi-Granular Text Encoding for Self-Explaining Categorization_. Zhiguo Wang, Yue Zhang, Mo Yu, Wei Zhang, Lin Pan, Linfeng Song, Kun Xu and Yousef El-Kurdi
- _The meaning of "most" for visual question answering models_. Alexander Kuhnle and Ann Copestake
- _Do Human Rationales Improve Machine Explanations?_. Julia Strout, Ye Zhang and Raymond Mooney
- _Analyzing the Structure of Attention in a Transformer Language Model_. Jesse Vig and Yonatan Belinkov
- _Detecting Political Bias in News Articles Using Headline Attention_. Rama Rohit Reddy Gangula, Suma Reddy Duggenpudi and Radhika Mamidi
- _Testing the Generalization Power of Neural Network Models Across NLI Benchmarks_. Aarne Talman and Stergios Chatzikyriakidis

Extended abstracts:
- _(Un)natural word-order biases in deep agent architectures_. Rahma Chaabouni, Evgeny Kharitonov, Emmanuel Dupoux and Marco Baroni
- _Why does a CNN predict this class? Interpreting Convolutional Neural Networks for Text Classification_. Piyawat Lertvittayakumjorn and Francesca Toni 
- _State-Regularized Recurrent Neural Networks_. Cheng Wang and Mathias Niepert 
- _Additional evidences that BERT learn syntactic structures_. Ganesh Jawahar, Benoît Sagot and Djamé Seddah 
- _Neural Networks as Explicit Word-Based Rules_. Jindřich Libovický 
- _Neural Pathways: A Method of Abstraction for Deep Neural Model Inspection and Comparison_. James Fiacco, Samridhi Choudhary and Carolyn Rose 
- _Can we Explain Natural Language Inference Decisions taken with Neural Networks? Inference Rules in Distributed Representations_. Fabio Massimo Zanzotto and Lorenzo Ferrone 
- _Visualizing Deep Neural Networks for Speech Recognition with Learned Topographic Filter Maps_. Andreas Krug and Sebastian Stober


Poster session 2
----------------
Archival papers:
- _Modeling Paths for Explainable Knowledge Base Completion_. Josua Stadelmaier and Sebastian Padó
- _Probing word and sentence embeddings for long-distance dependencies effects in French and English_. Paola Merlo
- _Derivational Morphological Relations in Word Embeddings_. Tomáš Musil, Jonáš Vidra and David MarecˇekMareček
- _Investigating sub-word embedding strategies in the morphologically rich and free phrase-order Hungarian_. Bálint Döbrössy, Márton Makrai, Balázs Tarján and György Szaszák
- _Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations_. Ethan Wilcox, Roger Levy and Richard Futrell
- _Blackbox meets blackbox: Representational Similarity & Stability Analysis of Neural Language Models and Brains_. Samira Abnar, Lisa Beinborn, Rochelle Choenni and Willem Zuidema
- _An LSTM adaptation study of (un)grammaticality_. Shammur Absar Chowdhury and Roberto Zamparelli
- _An Analysis of Source-Side Grammatical Errors in NMT_. Antonios Anastasopoulos
- _Finding Hierarchical Structure in Neural Stacks Using Unsupervised Parsing_. William Merrill, Lenny Khazan, Noah Amsel, Yiding Hao, Simon Mendelsohn and Robert Frank
- _Adversarial Attack on Sentiment Classification_. Yi-Ting Tsai, Min-Chu Yang and Han-Yu Cheni
- _Open Sesame: Getting Inside BERT’s Linguistic Knowledge_. Yongjie Lin, Yi Chern Tan and Robert Frank

Extended abstrcats:
- _Inducing syntactic trees from BERT representation_. Rudolf Rosa and David Mareček
- _Exploring Universal Sentence Encoders' ability to learn cause-and-effects reasoning using a new precedence classification probing task_. Yochay Gurman and Reut Tsarfaty
- _Interactive White-Box Models through Collaborative Semantic Inference_. Sebastian Gehrmann, Hendrik Strobelt, Robert Krüger, Hanspeter Pfister and Alexander Rush 
- _Do LSTMs Learn Long-Distance Dependencies From Constituents?_. Naomi Saphra and Adam Lopez
- _Learning to make accuracy judgments from intermediate and final outputs of a neural network for a question answering task_. Chad DeChant, Seungwook Han and Hod Lipson 
- _Some linguistic correlates of gradients and attention weights in BERT_. Matthijs Westera
- _Towards Understanding Position Embeddings_. Rishi Bommasani and Claire Cardie
- _On the Importance of Delexicalization for Fact Verification_. Sandeep Suntwal, Mithun Paul, Rebecca Sharp and Mihai Surdeanu






