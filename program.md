
[Main page](index.md)

Summary of the program
----------------------

| Time         | Program item                                  |
|--------------|-----------------------------------------------|
| 09:00-09:10  | Opening remarks                               |
| 09:10-10:00  | Invited talk 1: [Yoav Goldberg](index.md#yoav-goldberg)|
| 10:00-11:00  | Poster session 1 (10:30-11 tea break)         |
| 11:00-12:30  |  Oral presentation session 1 (6 x 15 minutes) |
| 12:30-14:00  | Lunch                                         |
| 14:00-14:50  | Invited talk 2: [Graham Neubig](index.md#graham-neubig)|
| 14:50-16:00  | Poster session 2 (15:30-16 tea break)         |
| 16:00-16:50  | Invited talk 3: [Leila Wehbe](index.md#leila-wehbe)|
| 16:50-17:20  |  Oral presentation session 2 (2 x 15 minutes) |
| 17:20-17:30  | Best paper announcement and closing remarks   |


Oral presentation session 1
---------------------------

- 11:00-11:15 _Interpretable Structure Induction via Sparse Attention_. Ben Peters, Vlad Niculae and André F. T. Martins.
- 11:15-11:30 _Understanding Convolutional Neural Networks for Text Classification_. Alon Jacovi, Yoav Goldberg and Oren Sar Shalom.
- 11:30-11:45 _Extracting Syntactic Trees from Transformer Encoder Self-Attentions_. David Mareček and Rudolf Rosa.
- 11:45-12:00 _Context-Free Transductions with Neural Stacks_. Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz and Simon Mendelsohn.
- 12:00-12:15 _Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures_. Danilo Croce, Daniele Rossini and Roberto Basili.
- 12:15-12:30 _Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell_. Pia Sommerauer and Antske Fokkens.

Oral presentation session 2
---------------------------

- 16:50-17:05 _What do RNN Language Models Learn about Filler–Gap Dependencies?_ Ethan Wilcox, Roger Levy, Takashi Morita and Richard Futrell. 
- 17:05-17:20 _Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information_. Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes and Willem Zuidema.

Poster session 1
----------------
- _When does deep multi-task learning work for loosely related document classification tasks?_	Emma Kerinec, Chloé Braud and Anders Søgaard.
- _Analyzing Learned Representations of a Deep ASR Performance Prediction Model._	Zied Elloumi, Laurent Besacier, Olivier Galibert and Benjamin Lecouteux.
- _Learning Explanations from Language Data._	David Harbecke, Robert Schwarzenberg and Christoph Alt.
- _Nightmare at test time: How punctuation prevents parsers from generalizing._	Anders Søgaard, Miryam de Lhoneux and Isabelle Augenstein.
- _How much should you ask? On the question structure in QA systems._	Barbara Rychalska, Dominika Basaj, Anna Wróblewska and Przemyslaw Biecek.
- _Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System._	Barbara Rychalska, Dominika Basaj, Anna Wróblewska and Przemyslaw Biecek.
- _Interpretable Textual Neuron Representations for NLP._	Nina Poerner, Benjamin Roth and Hinrich Schütze.
- _Evaluating Textual Representations through Image Generation._	Graham Spinks and Marie-Francine Moens.
- _On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis._	Jose Camacho-Collados and Mohammad Taher Pilehvar.
- _Language Models Learn POS First._	Naomi Saphra and Adam Lopez.
- _Jump to better conclusions: SCAN both left and right._	Joost Bastings, Marco Baroni, Jason Weston, Kyunghyun Cho and Douwe Kiela.
- _Linguistic representations in multi-task neural networks for ellipsis
resolution._	Ola Rønning, Daniel Hardt and Anders Søgaard.
- _Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models._	Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki Okazaki, Kentaro Inui and Masaaki Nagata.
- _Rule induction for global explanation of trained models._	Madhumita Sushil, Simon Suster and Walter Daelemans.
- _Predicting and interpreting embeddings for out of vocabulary words in downstream tasks._	Nicolas Garneau, Jean-Samuel Leboeuf and Luc Lamontagne.
- _Can LSTM Learn to Capture Agreement? The Case of Basque._	Shauli Ravfogel, Yoav Goldberg and Francis Tyers.
- _Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks._	Joao Loula, Marco Baroni and Brenden Lake.
- _Probing sentence embeddings for structure-dependent tense._	Geoff Bacon and Terry Regier.
- _Evaluating the Ability of LSTMs to Learn Context-Free Grammars._	Luzi Sennhauser and Robert Berwick.
- _Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation._	Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White and Benjamin Van Durme.
- _Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style._	Reid Pryzant, Sugato Basu and Kazoo Sone.
- _Interpretable Word Embedding Contextualization._	Kyoung-Rok Jang, Sung-Hyon Myaeng and Sang-Bum Kim.
- _Interpreting Neural Networks with Nearest Neighbors._	Eric Wallace, Shi Feng and Jordan Boyd-Graber.
- _'Indicatements' that character language models learn English morpho-syntactic units and regularities._	Yova Kementchedjhieva and Adam Lopez.
- _Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items._	Jaap Jumelet and Dieuwke Hupkes.

Poster session 2
----------------

- _State Gradients for RNN Memory Analysis._	Lyan Verwimp, Hugo Van hamme, Vincent Renkens and Patrick Wambacq.
- _LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation._	Pankaj Gupta and Hinrich Schütze.
- _Analysing the potential of seq-to-seq models for incremental interpretation in task-oriented dialogue._	Dieuwke Hupkes, Sanne Bouwmeester and Raquel Fernández.
- _An Operation Sequence Model for Explainable Neural Machine Translation._	Felix Stahlberg and Bill Byrne.
- _Introspection for convolutional automatic speech recognition._	Andreas Krug and Sebastian Stober.
- _Portable, layer-wise task performance monitoring for NLP models._	Tom Lippincott.
- _GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding._	Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy and Samuel Bowman.
- _Explicitly modeling case improves neural dependency parsing._	Clara Vania and Adam Lopez.
- _Learning and Evaluating Sparse Interpretable Sentence Embeddings._	Valentin Trifonov, Octavian-Eugen Ganea, Anna Potapenko and Thomas Hofmann.
- _Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis._	Kelly Zhang and Samuel Bowman.
- _Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model._	Steven Derby, Paul Miller, Brian Murphy and Barry Devereux.
- _Closing Brackets with Recurrent Neural Networks._	Natalia Skachkova, Thomas Trost and Dietrich Klakow.
- _Iterative Recursive Attention Model for Interpretable Sequence Classification._	Martin Tutek and Jan Šnajder.
- _Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models._	Avery Hiebert, Cole Peterson, Alona Fyshe and Nishant Mehta.
- _Debugging Sequence-to-Sequence Models with Seq2Seq-Vis._	Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister and Alexander Rush.
- _Grammar Induction with Neural Language Models: An Unusual Replication._	Phu Mon Htut, Kyunghyun Cho and Samuel Bowman.
- _Importance of Self-Attention for Sentiment Analysis._	Gaël Letarte, Frédérik Paradis, Philippe Giguère and François Laviolette.
- _Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?_	Prajit Dhar and Arianna Bisazza
- _Exploiting Attention to Reveal Shortcomings in Memory Models._ Kaylee Burns, Aida Nematzadeh, Erin Grant, Alison Gopnik and Tom Griffiths.
- _An Analysis of Encoder Representations in Transformer-Based Machine Translation._	Alessandro Raganato and Jörg Tiedemann.
- _End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space._	Pranava Swaroop Madhyastha, Josiah Wang and Lucia Specia.
- _Evaluating Grammaticality in Seq2seq Models with a Broad Coverage HPSG Grammar: A Case Study on Machine Translation._	Johnny Wei, Khiem Pham, Brendan O'Connor and Brian Dillon.
- _Limitations in learning an interpreted language with recurrent models._	Denis Paperno.

